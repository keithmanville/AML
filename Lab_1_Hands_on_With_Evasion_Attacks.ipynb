{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyO/m/9YPnhOQozweylnQJk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oaramoon/AML-Course/blob/main/Lab_1_Hands_on_With_Evasion_Attacks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lab 1: Hands-on With Evasion Attacks**\n",
        "\n",
        " In this lab, we delve into the world of adversarial machine learning by exploring evasion attacks, also known as adversarial example attacks. Our journey will encompass hands-on experiments and insights into how these attacks operate and their implications on machine learning models. Here is what we are going to cover:\n",
        "\n",
        "**1. Getting Familiar with the Target Model: ResNet50 Classifier**\n",
        "   - We will deploy a pre-trained ResNet50 classifier, a powerful and widely-used neural network, as our test subject. This model, proficient in identifying various objects from the extensive ImageNet dataset, serves as an ideal candidate for understanding evasion attacks.\n",
        "\n",
        "**2. Understanding Resilience to Random Perturbations**\n",
        "   - Our initial exploration begins with a key question: Does random noise always trick a neural network? We will experiment by introducing random perturbations to the input images and observe the ResNet50 classifier's performance. This exercise aims to demonstrate the innate resilience of neural networks against such random modifications.\n",
        "\n",
        "**3. White-Box Evasion Attacks**\n",
        "   - Moving deeper, we'll dissect white-box evasion attacks, specifically focusing on the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) techniques. Here, the 'white-box' context implies complete access to the model's architecture and parameters.\n",
        "   - Our goal is to demystify the complex mathematical formulas presented in lectures and translate them into executable code. By doing so, we'll gain a practical understanding of how targeted and untargeted attacks are implemented and their effects on the classifier.\n",
        "\n",
        "**4. Black-Box Threat Model**\n",
        "   - The final segment of our lab introduces the black-box threat model, where the attacker has no internal knowledge of the model but can only make queries and observe predictions.\n",
        "   - we'll apply gradient estimation methods to execute attacks on black-box models, illustrating a more realistic attack scenario where the adversary's knowledge is limited.\n",
        "   - Additionally, we will investigate the concept of adversarial example transferability - the phenomenon where adversarial examples crafted for one model can also deceive another model.\n",
        "   \n",
        "\n",
        "By the end of this lab, you will have a comprehensive understanding of how evasion attacks function, their impact on machine learning models, and the differences between white-box and black-box attack strategies. This knowledge is crucial for developing more robust and secure AI systems in the future."
      ],
      "metadata": {
        "id": "yY9GIsuqVLt4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1: Getting Familiar with the Target Model: ResNet50 Classifier**\n",
        "\n",
        "In this part of the lab, we will lay the foundation for our experiments with evasion attacks. We'll do this by setting up a pre-trained classifier and preparing our test image."
      ],
      "metadata": {
        "id": "Xloa7YfmeNZy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setup**\n",
        "\n",
        "Importing the packages we need for this lab."
      ],
      "metadata": {
        "id": "rEYkmqGRVXLb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, decode_predictions\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet50_preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "5SlmUJUkVQzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading Target Classifier**\n",
        "\n",
        "The next step is to acquire a powerful neural network model that we will attempt to attack. For this purpose, we'll use tensorflow, a popular deep learning library, to download a pre-trained ResNet50 classifier. This model has been trained on the ImageNet dataset, making it proficient in recognizing a wide array of objects. Running the code snippet below downloads the ResNet50 model directly from tensorflow's selection of pre-trained models and loads it into our Python environment."
      ],
      "metadata": {
        "id": "dQj1SwYvVae5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_classifier = ResNet50(weights='imagenet')\n",
        "target_classifier.summary()"
      ],
      "metadata": {
        "id": "hjZ4WdRWVawA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loading and Viewing Target Image**\n",
        "\n",
        "To conduct an evasion attack, we need an image to work with. This image will serve as the 'target' which we will attempt to manipulate.\n",
        "We will download a specific image that we want to use for our experiment.\n",
        "Once downloaded, we'll display this image in our environment, allowing us to visually inspect the original unaltered state of our target."
      ],
      "metadata": {
        "id": "8-hANTvuVbBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_image(img_path):\n",
        "    img = image.load_img(img_path, target_size=(224, 224))\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n",
        "\n",
        "\n",
        "# Download image\n",
        "!wget https://raw.githubusercontent.com/oaramoon/AML-Course/main/labrador_retriever.png -O labrador_retriever.png\n",
        "\n",
        "img_path = './labrador_retriever.png'  # Replace with your image path\n",
        "original_img = load_image(img_path)\n",
        "\n",
        "\n",
        "# Display the original image\n",
        "def display_image_w_prediction(img, model,preprocess_input):\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(7, 7))\n",
        "    ax.imshow(np.squeeze(img).astype(\"uint8\"))\n",
        "\n",
        "    pred = model.predict(preprocess_input(img.copy()),verbose=0)\n",
        "    (_,label, conf) = decode_predictions(pred, top=1)[0][0]\n",
        "\n",
        "    ax.set_title(f'Predicted Label: \"{label.replace(\"_\",\" \")} w/ {100*conf:.2f}% confidence\"')\n",
        "    ax.axis('off')\n",
        "\n",
        "display_image_w_prediction(img=original_img, model=target_classifier, preprocess_input=resnet50_preprocess_input)\n"
      ],
      "metadata": {
        "id": "ecQI-E4wVbJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Does the target image look familiar?"
      ],
      "metadata": {
        "id": "AwTDfLva3Tlq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Understanding Resilience to Random Perturbations**\n",
        "\n",
        "The crux of an evasion attack lies in subtly altering the input image in a way that causes the model to misclassify it. This alteration is known as 'adversarial noise'. One (or poeple who were sleeping during Lecture 3) might think that adversarial noise can be found through a trial-and-error process, randomly sampling a perturbation within a permitted range and testing its ability to fool the classifier. This process would be repeated until an effective perturbation is discovered.\n",
        "\n",
        "Let's implement this approach! Make 1000 quries to the model where the target iag is superimpose by a random noise and record how many times you were successding to fool the model.\n",
        "\n",
        "**Implementation Steps:**\n",
        "\n",
        "- **Noise Generation**: For each query (out of 1000 total), generate a unique random noise pattern. This noise must be within a permissible range so that it doesn't drastically alter the image's appearance. We will assume a 'permissible range' for our noise, defined by an L_infinity norm of 3. This means the maximum change allowed for any pixel value in the image will not exceed 3 units in any direction (positive or negative). This constraint ensures that our perturbations are subtle and do not drastically alter the overall appearance of the image. Note that pixle values are in [0,255].\n",
        "\n",
        "- **Superimposing Noise:** Apply the generated noise to the target image, creating a potentially adversarial example.\n",
        "\n",
        "- **Model Querying:** Feed this modified image to our pre-trained ResNet50 classifier and record the model's prediction.\n",
        "\n",
        "- **Success Evaluation:** Determine if the noise led to a misclassification. If the model's prediction differs from the original, unaltered image's classification, count it as a success.\n",
        "\n",
        "Keep track of the number of successful misclassifications out of the 1000 attempts. This will give us a quantitative measure of how effective this brute-force method is in crafting adversarial examples.\n"
      ],
      "metadata": {
        "id": "FUe3uu2J1G06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def adv_noise_by_trial_and_error(model=target_classifier, preprocess_input=resnet50_preprocess_input, num_attempts=1000, l_inf=3): # this function is supposed to return attack_success_rate\n",
        "  num_fooled = 0\n",
        "\n",
        "  ## WRITE YOUR CODE BELOW\n",
        "  #....\n",
        "  #\n",
        "  #return num_fooled/num_attempts\n",
        "\n",
        "attack_success_rate = adv_noise_by_trial_and_error():\n",
        "print(f\"{100*attack_success_rate:.2f}% of queries resulted in misclassification.\")"
      ],
      "metadata": {
        "id": "nIwD22gF4BA4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3: White-box Evasion Attacks**\n",
        "\n",
        "Having experienced the limitations of a trial-and-error approach in generating adversarial examples, we now shift our focus to a more strategic and informed method. In this part of the lab, we will delve into the realm of white-box attacks, where the adversary is assumed to have complete knowledge of the machine learning model.\n",
        "\n",
        "Our first goal is to review the implmentation of an untargeted FGSM (Fast Gradient Sign Method) attack, a technique we discussed in our lectures. Despite its effectiveness, you might be surprised to discover how straightforward it is to implement FGSM; it requires only a few lines of code.\n",
        "\n",
        "**Implementation Steps:**\n",
        "- **Gradient Calculation**: In the first code block, function `get_loss_gradient_wrt_input` implements calculating the gradient of the loss with respect to the input image. This step is crucial as it determines the direction in which we will perturb our image.\n",
        "- **Creating the Adversarial Example**: In the first code block, function `untargeted_fgsm_attack` applies the FGSM method using the calculated gradient. We will adjust the image pixels slightly in the direction of the gradient's sign, staying within our predefined L infinity norm constraint of 3.\n",
        "- **Visualizing the Result**: In the second code block, after generating the adversarial image, we will display it alongside the original image for comparison. This visual representation will help you understand the subtlety of the changes made and their impact on the classifier's prediction."
      ],
      "metadata": {
        "id": "8zuW8Azu08Oy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Untargeted FGSM** **Attack**"
      ],
      "metadata": {
        "id": "NNnD3eEVVbSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def get_loss_gradient_wrt_input(x, y, target_model):\n",
        "    # Define the loss function (Categorical Crossentropy for multi-class classification)\n",
        "    training_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    # Use TensorFlow's GradientTape to record operations for automatic differentiation\n",
        "    with tf.GradientTape() as tape:\n",
        "        tape.watch(x)  # Ensure the tape is watching the input tensor 'x'\n",
        "        prediction = target_model(x)  # Obtain the model's prediction for the input 'x'\n",
        "        loss = training_loss(tf.one_hot([y], 1000), prediction)  # Calculate the loss between the true label 'y' and prediction\n",
        "\n",
        "    gradient = tape.gradient(loss, x)  # Compute the gradient of the loss with respect to the input image 'x'\n",
        "    return gradient  # Return the gradient\n",
        "\n",
        "def untargeted_fgsm_attack(target_model, img, img_label, epsilon, preprocess_input):\n",
        "    img_tensor = tf.convert_to_tensor(img)  # Convert the image to a TensorFlow tensor\n",
        "\n",
        "    # Obtain the gradient of the loss with respect to the input image\n",
        "    grads = get_loss_gradient_wrt_input(target_model=target_model, x=preprocess_input(img_tensor), y=img_label)\n",
        "\n",
        "    sign_grads = tf.sign(grads)  # Get the sign of the gradient (to determine the direction of the perturbation)\n",
        "\n",
        "    # Create the perturbed image by adding the scaled gradient (epsilon * sign_grads) to the original image\n",
        "    perturbed_image = img_tensor + epsilon * sign_grads\n",
        "\n",
        "    # Ensure that the pixel values of the perturbed image remain within valid range (0-255)\n",
        "    perturbed_image = tf.clip_by_value(perturbed_image, 0, 255)\n",
        "\n",
        "    return perturbed_image.numpy()  # Convert the tensor back to a numpy array and return\n",
        "\n",
        "\n",
        "# Ensure preprocessed_img is a NumPy array, not a tensor\n",
        "untargeted_fgsm_adv_img = untargeted_fgsm_attack(target_model=target_classifier, img=original_img, img_label=208, epsilon=3, preprocess_input=resnet50_preprocess_input)  # 208 is the class index for 'Labrador Retriever'"
      ],
      "metadata": {
        "id": "wWHKfHHiVbYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_adversarial_and_original_images(original, adversarial, model, preprocess_input):\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(30, 10))  # Adjusted for three subplots\n",
        "\n",
        "    # Original and adversarial predictions\n",
        "    original_pred = model.predict(preprocess_input(original.copy()),verbose=0)\n",
        "    adversarial_pred = model.predict(preprocess_input(adversarial.copy()),verbose=0)\n",
        "    (_, orig_label, org_conf) = decode_predictions(original_pred, top=1)[0][0]\n",
        "    (_, adv_label, adv_conf) = decode_predictions(adversarial_pred, top=1)[0][0]\n",
        "\n",
        "    # Display original image\n",
        "    axes[0].imshow(np.squeeze(original.astype('uint8')))\n",
        "    axes[0].set_title(f'Original: \"{orig_label.replace(\"_\",\" \")}\" ({100*org_conf:.2f}%)')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Display adversarial image\n",
        "    axes[1].imshow(np.squeeze(adversarial.astype('uint8')))\n",
        "    axes[1].set_title(f'Adversarial: \"{adv_label.replace(\"_\",\" \")}\" ({100*adv_conf:.2f}%)')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Calculate and display the perturbation\n",
        "    perturbation = adversarial - original\n",
        "    perturbation = (perturbation - perturbation.min()) / (perturbation.max() - perturbation.min())  # Normalize\n",
        "    axes[2].imshow(np.squeeze(perturbation).astype('float32'))\n",
        "    axes[2].set_title('Adversarial Perturbation')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "display_adversarial_and_original_images(original=original_img, adversarial=untargeted_fgsm_adv_img, model=target_classifier, preprocess_input=resnet50_preprocess_input)"
      ],
      "metadata": {
        "id": "YgrE0LDMebqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Targeted FGSM Attack**\n",
        "\n",
        "Now that you've gained hands-on experience with an untargeted FGSM attack, it's time to take it a step further. In this challenge, you are tasked with implementing a targeted version of the FGSM attack. Your goal is to manipulate the input image in such a way that the pre-trained ResNet50 classifier is not just fooled into making any wrong prediction, but specifically misclassifies the image as a 'Tree Frog' (label index 31 in the ImageNet dataset).\n",
        "\n",
        "After implementing these changes, test your attack on the same image used previously. Observe if the classifier now misclassifies it as a 'Tree Frog'."
      ],
      "metadata": {
        "id": "lKEZBP8MbSZs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def targeted_fgsm_attack(target_model, img, target_label, epsilon, preprocess_input):\n",
        "    ## WRITE YOUR CODE BELOW\n",
        "    #....\n",
        "    #\n",
        "    #return perturbed_image.numpy()\n",
        "\n",
        "\n",
        "targeted_fgsm_adv_img = targeted_fgsm_attack(target_model=target_classifier, img=original_img, target_label=31, epsilon=3, preprocess_input=resnet50_preprocess_input) # 31 is the class index for 'Tree Frog'\n",
        "display_adversarial_and_original_images(original=original_img, adversarial=targeted_fgsm_adv_img, model=target_classifier, preprocess_input=resnet50_preprocess_input)"
      ],
      "metadata": {
        "id": "TFQ5lp6obTyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your targeted FGSM attack didn't succeed in fooling the classifier into misclassifying the image as a 'Tree Frog', It was expected.\n",
        "The FGSM, while powerful, has certain limitations, especially in the context of targeted attacks. FGSM is known for its simplicity and efficiency in generating adversarial examples quickly. However, this simplicity can be a drawback when precision and specific target misclassifications are required. Targeted attacks generally demand a more refined approach than what FGSM offers. FGSM makes a single, large step in the direction of the gradient sign, which might not be subtle or precise enough to fool a model into misclassifying an image as a specific class, especially when the target class is significantly different from the true class.\n",
        "\n",
        "To increase the success rate of targeted attacks, one might consider iterative methods like the Projected Gradient Descent (PGD), which takes smaller steps towards the target but iterates multiple times. This approach can offer a more refined control over the adversarial example generation."
      ],
      "metadata": {
        "id": "7KGt1D_9AmBq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Targeted PGD Attack**\n",
        "\n",
        "Having explored the Fast Gradient Sign Method (FGSM) and its limitations in targeted attacks, we now turn our attention to a more sophisticated technique: the Projected Gradient Descent (PGD) attack. PGD is known for its effectiveness in crafting more effective adversarial examples, especially in targeted scenarios.\n",
        "\n",
        "Your task is to implement a targeted PGD attack. The goal remains the same as with the FGSM challenge: to manipulate an input image so that our pre-trained model misclassifies it as a 'Tree Frog' (label index 31 in the ImageNet dataset).\n",
        "As before, the permissible range for perturbation (defined by the L infinity\n",
        "norm) will be the same.\n",
        "\n",
        "**Implementation Guidance:**\n",
        "\n",
        "- Start with the Original Image: Initialize your attack with the original image you used in the FGSM experiment.\n",
        "\n",
        "- Iterative Perturbation: In each iteration, apply a small perturbation in the direction that increases the likelihood of the image being classified as the target class (Tree Frog).\n",
        "-Apply Projection: After each perturbation, use a projection function to ensure that the adversarial example stays within the permissible range.\n",
        "- Ensure that the pixel values of the perturbed image remain within valid range (0-255)  \n",
        "Repeat: Continue this process for a set number of iterations or until the image is successfully classified as a Tree Frog.\n",
        "\n",
        "\n",
        "After completing the iterations, evaluate whether the model now misclassifies the altered image as a 'Tree Frog'."
      ],
      "metadata": {
        "id": "tk_gWTDAcLJa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def targeted_pgd_attack(target_model, img, target_label, epsilon, preprocess_input, num_ittr=50):\n",
        "    ## WRITE YOUR CODE BELOW\n",
        "    #....\n",
        "    #\n",
        "    #return perturbed_image.numpy()\n",
        "\n",
        "\n",
        "targeted_pgd_adv_img = targeted_pgd_attack(target_model=target_classifier, img=original_img, target_label=31, epsilon=3, preprocess_input=resnet50_preprocess_input)\n",
        "display_adversarial_and_original_images(original=original_img, adversarial=targeted_pgd_adv_img, model=target_classifier, preprocess_input=resnet50_preprocess_input)\n"
      ],
      "metadata": {
        "id": "7-hBUocCcKJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2: Black-box Evasion Attacks**\n",
        "In the real world, adversaries often do not have full access to a target model's internal parameters and architecture. This is known as a black-box attack scenario. Unlike white-box attacks where the attacker has complete knowledge of the model, black-box attacks present unique challenges. In black-box settings, adversaries cannot directly compute the gradients of the adversarial loss since, by just running a few lines of code, as they lack access to the model's internal workings. This makes generating adversarial examples more complex and requires different strategies. Here we review two of such strategies.\n",
        "\n",
        "\n",
        "If the adversary has access to the confidence scores output by the target model for each prediction, they can use these scores to their advantage through **Zeroth-Order Optimization Methods**. These methods do not require explicit gradient computation. Instead, they estimate gradients based on the output scores provided by the model. This approach is particularly useful in black-box scenarios."
      ],
      "metadata": {
        "id": "dQ0nCF2fdki9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Untargeted Black-box FGSM Attack via Zeroth-Order Methods**\n",
        "\n",
        " Zeroth-order methods approximate the gradient by observing changes in the model's output (confidence scores) in response to slight variations in the input. By systematically tweaking the input and noting the corresponding changes in output, these methods can infer the direction in which to modify the input to induce misclassification. This involves running a series of queries to the target model with slightly altered inputs and analyzing the resulting confidence scores. Based on these observations, the adversary can estimate the gradient and use it to craft an adversarial example.\n",
        "\n",
        "In the next code block, we'll demonstrate an implementation of an untargeted black-box FGSM attack using a zeroth-order estimation method, namely the **difference quotient** method which we all learne din highschool. Function `estimate_gradient` estimates the gradient along a specific pixel (denoted by the index variable), and `fgsm_attack_with_estimated_gradients` implements the implements the black-box FGSM attack.\n",
        "\n",
        "Before running the code block, it's important to set realistic expectations regarding execution time. Due to the iterative and computationally demanding nature of zeroth-order methods, especially when applied to high-resolution images, the processing time can be substantial. This significant time requirement is one of the drawbacks of using zeroth-order methods in adversarial attacks. This exercise serves as an important lesson in the practical challenges adversaries face in real-world scenarios. It underscores the need for balancing precision and computational efficiency in adversarial strategies."
      ],
      "metadata": {
        "id": "8Gz6w_AWidA6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_gradient(x, y, index, target_model, preprocess_input, h=1e-4):\n",
        "    # Categorical Crossentropy loss for multi-class classification\n",
        "    training_loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "\n",
        "    x_tensor = tf.convert_to_tensor(x)  # Convert the image to a TensorFlow tensor\n",
        "\n",
        "    # Save the original value at the specified index (pixel)\n",
        "    orig_val = x_tensor[index]\n",
        "\n",
        "    # Perturb x at the specified index by a small value +h and calculate the loss\n",
        "    x_plus_h = tf.tensor_scatter_nd_update(x_tensor, [index], [orig_val + h])\n",
        "    loss_plus_h = training_loss(tf.one_hot([y], 1000), target_model(preprocess_input(x_plus_h)))\n",
        "\n",
        "    # Perturb x at the specified index by a small value -h and calculate the loss\n",
        "    x_minus_h = tf.tensor_scatter_nd_update(x_tensor, [index], [orig_val - h])\n",
        "    loss_minus_h = training_loss(tf.one_hot([y], 1000), target_model(preprocess_input(x_minus_h)))\n",
        "\n",
        "    # Estimate the gradient using the central difference quotient formula\n",
        "    gradient = (loss_plus_h - loss_minus_h) / (2 * h)\n",
        "    return gradient  # Return the estimated gradient\n",
        "\n",
        "def fgsm_attack_with_estimated_gradients(target_model, img, img_label, epsilon, preprocess_input, h=1e-4):\n",
        "    start_time = time.time()  # Record the start time of the attack\n",
        "    img_tensor = tf.convert_to_tensor(img)  # Convert the image to a TensorFlow tensor\n",
        "    perturbed_image = tf.identity(img_tensor)  # Create a copy of the image tensor\n",
        "\n",
        "    num_pixels_covered = 0  # Track the number of pixels processed\n",
        "\n",
        "    # Iterate over each pixel in the image to estimate the gradient\n",
        "    for i in (range(img_tensor.shape[1])):  # Iterating over height\n",
        "        for j in (range(img_tensor.shape[2])):  # Iterating over width\n",
        "            for c in (range(img_tensor.shape[3])):  # Iterating over channels\n",
        "                index = [0, i, j, c]  # Create an index for the current pixel\n",
        "\n",
        "                # Estimate the gradient at the current pixel\n",
        "                grad_est = estimate_gradient(x=img_tensor, y=img_label, index=index, target_model=target_model, h=h, preprocess_input=preprocess_input)\n",
        "                sign_grad = tf.sign(grad_est)  # Get the sign of the estimated gradient\n",
        "\n",
        "                # Update the perturbed image at the current pixel with the calculated gradient\n",
        "                perturbed_image_val = perturbed_image[index] + epsilon * sign_grad\n",
        "                perturbed_image = tf.tensor_scatter_nd_update(perturbed_image, [index], [perturbed_image_val])\n",
        "\n",
        "                # Calculate and print the runtime and progress\n",
        "                duration = time.time() - start_time\n",
        "                num_pixels_covered += 1\n",
        "                print(f\"Runtime: {int(duration // 3600):02}:{int((duration % 3600) // 60):02}:{int(duration % 60):02}, Progress: {(num_pixels_covered*100)/(img_tensor.shape[1]*img_tensor.shape[2]*img_tensor.shape[3]):.3f}%\")\n",
        "\n",
        "    # Ensure the pixel values of the perturbed image remain within the valid range (0-255)\n",
        "    perturbed_image = tf.clip_by_value(perturbed_image, 0, 255)\n",
        "    return perturbed_image.numpy()  # Convert the tensor back to a numpy array and return\n",
        "\n",
        "\n",
        "\n",
        "black_box_untargeted_fgsm_adv_img = fgsm_attack_with_estimated_gradients(target_model=target_classifier, img=original_img, img_label=208, epsilon=1, preprocess_input=resnet50_preprocess_input)  # 208 is the class index for 'Labrador Retriever'\n",
        "display_adversarial_and_original_images(original=original_img, adversarial=black_box_untargeted_fgsm_adv_img, model=target_classifier, preprocess_input=resnet50_preprocess_input)"
      ],
      "metadata": {
        "id": "Zrekn9F7g1as"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Transferbility-based Black-box Attacks**\n",
        "\n",
        "\n",
        "Another effective strategy for mounting black-box evasion attacks can be leveraging the concept of 'transferability'. This approach relies on the phenomenon where adversarial examples crafted for one model can also be effective against another model.\n",
        "\n",
        "The core idea behind transferability is to use a surrogate model — a different model than the actual target, but trained for the same task. The adversary generates adversarial examples using this surrogate model, and then tests these examples on the actual target model. The rationale behind this strategy is that many machine learning models, despite being architecturally different, often learn similar decision boundaries for a given task. Therefore, an example that is adversarial for one model has a reasonable chance of being adversarial for another.\n",
        "\n",
        "In this part of the lab, we delve into the practical aspects of transferability in black-box attacks. We'll demonstrate how an adversarial example, crafted using one model, can potentially deceive another, seemingly unrelated model.\n",
        "\n",
        "\n",
        "For our exercise, we'll use the VGG16 model, a well-known convolutional neural network architecture. VGG16, like our target model ResNet50, is trained on the ImageNet dataset, making it a suitable choice for our surrogate. We'll obtain the VGG16 model directly from TensorFlow's library of pre-trained models.\n",
        "\n",
        "With the surrogate model in hand, we'll employ our untargeted white-box FGSM attack method. This involves creating an input that is purposely modified to cause the VGG16 model to misclassify it.\n",
        "\n",
        "Then we will see if the adversarial example, which is misleading for VGG16, will also fool the ResNet50 model."
      ],
      "metadata": {
        "id": "qcffuO7A6030"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input as vgg16_preprocess_input\n",
        "\n",
        "surrogate_classifier = VGG16(weights='imagenet')\n",
        "\n",
        "untargeted_transfer_fgsm_adv_img = untargeted_fgsm_attack(target_model=surrogate_classifier, img=original_img, img_label=208, epsilon=3, preprocess_input=vgg16_preprocess_input)  # 208 is the class index for 'Labrador Retriever'\n",
        "display_adversarial_and_original_images(original=original_img, adversarial=untargeted_transfer_fgsm_adv_img, model=target_classifier, preprocess_input=resnet50_preprocess_input)"
      ],
      "metadata": {
        "id": "VyRl5XU369JM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having explored transferability with untargeted FGSM attacks, we now raise the bar. Your task is to repeat the process, but this time using a more sophisticated method - the targeted Projected Gradient Descent (PGD) attack.\n",
        "Pay close attention to whether the target model misclassifies the image, and if so, whether it aligns with the target class specified in your PGD attack.\n",
        "\n"
      ],
      "metadata": {
        "id": "LRZ4D0Czegc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## WRITE YOUR CODE BELOW\n",
        "  #....\n",
        "  #"
      ],
      "metadata": {
        "id": "CMY8m1sHfukQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Did the target label succesfully transferred to the target classifier Transferring targeted adversarial examples between models is inherently more challenging than untargeted examples. This is because targeted attacks require manipulating the model’s output to a specific, often less probable class, which might not align with the decision boundaries of a different model.\n",
        "Learning Point: This exercise will help you understand the nuances and limitations of adversarial example transferability, especially in the context of targeted attacks. It highlights the complex interplay between the specificity of the attack and the generalizability of its effectiveness across different models"
      ],
      "metadata": {
        "id": "STe0F5_6gKVo"
      }
    }
  ]
}